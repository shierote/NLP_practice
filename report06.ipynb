{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問68<br>\n",
    "英語の文書のcos類似度の比較\n",
    "uberについて書かれた記事2本とlyftについて書かれた記事1本の類似度を比較してみる。\n",
    "- [uber1.txt](https://medium.com/free-code-camp/dark-genius-how-programmers-at-uber-volkswagen-and-zenefits-helped-their-employers-break-the-law-b7a7939c6591)\n",
    "\n",
    "- [uber2.txt](https://medium.com/sandpapersuit/side-hustle-as-a-sign-of-the-apocalypse-e7027a889fc2)\n",
    "\n",
    "- [lyft1.txt](https://medium.com/@johnzimmer/all-lyft-rides-are-now-carbon-neutral-55693af04f36)\n",
    "\n",
    "文章の量を一致させるため一部削っている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taishieguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from functools import reduce\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word_list(file_name):\n",
    "    word_list1 = []\n",
    "    with open(file_name) as f:\n",
    "        for s_line in f:\n",
    "            word_list1.append(nltk.word_tokenize(s_line))\n",
    "    word_list1 = reduce(lambda a, b: a + b, word_list1)\n",
    "    return np.array(word_list1)\n",
    "\n",
    "def count_f(word_list, all_word_list):\n",
    "    count_list = []\n",
    "    for i in range(all_word_list.size):\n",
    "        word = all_word_list[i]\n",
    "        count = np.count_nonzero(word_list == word)\n",
    "#         t1 = (word, count)\n",
    "        t1 = count\n",
    "        count_list.append(t1)\n",
    "    return np.array(count_list)\n",
    "\n",
    "def cos(f, g):\n",
    "    return np.dot(f, g)/(np.linalg.norm(f)*np.linalg.norm(g))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber1_list = word_list('uber1.txt')\n",
    "uber2_list = word_list('uber2.txt')\n",
    "lyft1_list = word_list('lyft1.txt')\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, uber1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6395783804293877"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, uber2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849740392966016"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber2_vec, lyft1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263543672138268"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, lyft1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uber1とuber2の値が近くなることを期待したが精度はイマイチであった（むしろlyftとuberの方が近づいてしまっている）。\n",
    "\n",
    "別の文書(newstimesの記事)で比較してみる。\n",
    "\n",
    "- [realestate1.txt](https://medium.com/free-code-camp/dark-genius-how-programmers-at-uber-volkswagen-and-zenefits-helped-their-employers-break-the-law-b7a7939c6591)\n",
    "\n",
    "- [realestate2.txt](https://medium.com/sandpapersuit/side-hustle-as-a-sign-of-the-apocalypse-e7027a889fc2)\n",
    "\n",
    "- [sports1.txt](https://medium.com/@johnzimmer/all-lyft-rides-are-now-carbon-neutral-55693af04f36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "realestate1_list = word_list('realestate1.txt')\n",
    "realestate2_list = word_list('realestate2.txt')\n",
    "sports1_list = word_list('sports1.txt')\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec = count_f(realestate1_list, all_word_list)\n",
    "realestate2_vec = count_f(realestate2_list, all_word_list)\n",
    "sports1_vec = count_f(sports1_list, all_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, realestate1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383622904249884"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, realestate2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632852370301972"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, sports1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255375589022625"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate2_vec, sports1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不動産の記事とスポーツの記事だが、きちんとその類似度を分類できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りの課題については締め切りに間に合わなかったため以下のリンクに貼った。<br>\n",
    "https://github.com/shierote/NLP_practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでcommon wordを取り除いて考えてみる。<br>\n",
    "common wordのリストは以下のサイトに載っているものを利用した。<br>\n",
    "https://www.ef.com/wwen/english-resources/english-vocabulary/top-1000-words/\n",
    "\n",
    "またカンマや括弧などの記号、文書の類似度に関係のなさそうな数字も目立ったためこれらも取り除いてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taishieguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from functools import reduce\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word_list(file_name):\n",
    "    word_list1 = []\n",
    "    with open(file_name) as f:\n",
    "        for s_line in f:\n",
    "            word_list1.append(nltk.word_tokenize(s_line))\n",
    "    word_list1 = reduce(lambda a, b: a + b, word_list1)\n",
    "    return np.array(word_list1)\n",
    "\n",
    "def count_f(word_list, all_word_list):\n",
    "    count_list = []\n",
    "    for i in range(all_word_list.size):\n",
    "        word = all_word_list[i]\n",
    "        count = np.count_nonzero(word_list == word)\n",
    "#         t1 = (word, count)\n",
    "        t1 = count\n",
    "        count_list.append(t1)\n",
    "    return np.array(count_list)\n",
    "\n",
    "def cos(f, g):\n",
    "    return np.dot(f, g)/(np.linalg.norm(f)*np.linalg.norm(g))\n",
    "\n",
    "def abstract_not_common_word(target_list):\n",
    "    common_words_list = word_list('common_words.txt')\n",
    "    res_list = []\n",
    "    for i in target_list:\n",
    "        res_list.append(not(i in common_words_list) and i.isalpha())\n",
    "        \n",
    "    return target_list[res_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "realestate1_list = word_list('realestate1.txt')\n",
    "abstract_not_common_word(realestate1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "realestate1_list = word_list('realestate1.txt')\n",
    "realestate2_list = word_list('realestate2.txt')\n",
    "sports1_list = word_list('sports1.txt')\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec = count_f(realestate1_list, all_word_list)\n",
    "realestate2_vec = count_f(realestate2_list, all_word_list)\n",
    "sports1_vec = count_f(sports1_list, all_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り除かなかった場合\n",
      "cos(realestate1_vec, realestate1_vec): 1.0\n",
      "cos(realestate1_vec, realestate2_vec): 0.8383622904249884\n",
      "cos(realestate2_vec, sports1_vec): 0.7255375589022625\n",
      "cos(realestate1_vec, sports1_vec): 0.7632852370301972\n"
     ]
    }
   ],
   "source": [
    "print('取り除かなかった場合')\n",
    "print(\"cos(realestate1_vec, realestate1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, realestate2_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate2_vec))\n",
    "\n",
    "print(\"cos(realestate2_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate2_vec, sports1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, sports1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り除いた場合\n",
      "cos(realestate1_vec, realestate1_vec): 1.0\n",
      "cos(realestate1_vec, realestate2_vec): 0.4483906358282693\n",
      "cos(realestate2_vec, sports1_vec): 0.11821621346237848\n",
      "cos(realestate1_vec, sports1_vec): 0.1559997520720196\n"
     ]
    }
   ],
   "source": [
    "realestate1_list = abstract_not_common_word(word_list('realestate1.txt'))\n",
    "realestate2_list = abstract_not_common_word(word_list('realestate2.txt'))\n",
    "sports1_list = abstract_not_common_word(word_list('sports1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec = count_f(realestate1_list, all_word_list)\n",
    "realestate2_vec = count_f(realestate2_list, all_word_list)\n",
    "sports1_vec = count_f(sports1_list, all_word_list)\n",
    "print('取り除いた場合')\n",
    "print(\"cos(realestate1_vec, realestate1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, realestate2_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate2_vec))\n",
    "\n",
    "print(\"cos(realestate2_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate2_vec, sports1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, sports1_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common wordを取り除くとその差がより明確になった。<br>\n",
    "uberとlyftでも検証してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り除かなかった場合\n",
      "cos(uber1_vec, uber1_vec): 1.0\n",
      "cos(uber1_vec, uber2_vec): 0.6395783804293877\n",
      "cos(uber2_vec, lyft1_vec): 0.6849740392966016\n",
      "cos(uber1_vec, lyft1_vec): 0.7263543672138268\n"
     ]
    }
   ],
   "source": [
    "uber1_list = word_list('uber1.txt')\n",
    "uber2_list = word_list('uber2.txt')\n",
    "lyft1_list = word_list('lyft1.txt')\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))\n",
    "print('取り除かなかった場合')\n",
    "print(\"cos(uber1_vec, uber1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, uber2_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber2_vec))\n",
    "\n",
    "print(\"cos(uber2_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber2_vec, lyft1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, lyft1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り除いた場合\n",
      "cos(uber1_vec, uber1_vec): 0.9999999999999999\n",
      "cos(uber1_vec, uber2_vec): 0.29304356514021723\n",
      "cos(uber2_vec, lyft1_vec): 0.32191127046385176\n",
      "cos(uber1_vec, lyft1_vec): 0.24494958586944585\n"
     ]
    }
   ],
   "source": [
    "uber1_list = abstract_not_common_word(word_list('uber1.txt'))\n",
    "uber2_list = abstract_not_common_word(word_list('uber2.txt'))\n",
    "lyft1_list = abstract_not_common_word(word_list('lyft1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))\n",
    "print('取り除いた場合')\n",
    "print(\"cos(uber1_vec, uber1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, uber2_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber2_vec))\n",
    "\n",
    "print(\"cos(uber2_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber2_vec, lyft1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, lyft1_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idfを使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taishieguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from functools import reduce\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word_list(file_name):\n",
    "    word_list1 = []\n",
    "    with open(file_name) as f:\n",
    "        for s_line in f:\n",
    "            word_list1.append(nltk.word_tokenize(s_line))\n",
    "    word_list1 = reduce(lambda a, b: a + b, word_list1)\n",
    "    return np.array(word_list1)\n",
    "\n",
    "def count_f(word_list, all_word_list):\n",
    "    count_list = []\n",
    "    for i in range(all_word_list.size):\n",
    "        word = all_word_list[i]\n",
    "        count = np.count_nonzero(word_list == word)\n",
    "#         t1 = (word, count)\n",
    "        t1 = count\n",
    "        count_list.append(t1)\n",
    "    return np.array(count_list)\n",
    "\n",
    "def cos(f, g):\n",
    "    return np.dot(f, g)/(np.linalg.norm(f)*np.linalg.norm(g))\n",
    "\n",
    "def abstract_not_common_word(target_list):\n",
    "    common_words_list = word_list('common_words.txt')\n",
    "    res_list = []\n",
    "    for i in target_list:\n",
    "        res_list.append(not(i in common_words_list) and i.isalpha())\n",
    "        \n",
    "    return target_list[res_list]\n",
    "\n",
    "def idf(list1, list2, list3):\n",
    "    for i in range(list1.size):\n",
    "        c = 0.0\n",
    "        if list1[i] > 0:\n",
    "            c += 1.0\n",
    "        if list2[i] > 0:\n",
    "            c += 1.0\n",
    "        if list3[i] > 0:\n",
    "            c += 1.0\n",
    "        list1[i] = list1[i]*np.log((c+1)/(3+1))\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uber2とlyft1が近づいてしまった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idfをつけなかった場合\n",
      "cos(uber1_vec, uber1_vec): 0.9999999999999999\n",
      "cos(uber1_vec, uber2_vec): 0.29304356514021723\n",
      "cos(uber2_vec, lyft1_vec): 0.32191127046385176\n",
      "cos(uber1_vec, lyft1_vec): 0.24494958586944585\n",
      "\n",
      "idfをつけた場合\n",
      "cos(uber1_vec, uber1_vec): 1.0\n",
      "cos(uber1_vec, uber2_vec): 0.05913123959890826\n",
      "cos(uber2_vec, lyft1_vec): 0.11809437324333252\n",
      "cos(uber1_vec, lyft1_vec): 0.008534859274986486\n"
     ]
    }
   ],
   "source": [
    "uber1_list = abstract_not_common_word(word_list('uber1.txt'))\n",
    "uber2_list = abstract_not_common_word(word_list('uber2.txt'))\n",
    "lyft1_list = abstract_not_common_word(word_list('lyft1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))\n",
    "print('idfをつけなかった場合')\n",
    "print(\"cos(uber1_vec, uber1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, uber2_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, uber2_vec))\n",
    "\n",
    "print(\"cos(uber2_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber2_vec, lyft1_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber1_vec, lyft1_vec))\n",
    "\n",
    "uber1_list = abstract_not_common_word(word_list('uber1.txt'))\n",
    "uber2_list = abstract_not_common_word(word_list('uber2.txt'))\n",
    "lyft1_list = abstract_not_common_word(word_list('lyft1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))\n",
    "\n",
    "uber1_idf_vec = idf(uber1_vec, uber2_vec, lyft1_vec)\n",
    "uber2_idf_vec = idf(uber2_vec, uber1_vec, lyft1_vec)\n",
    "lyft1_idf_vec = idf(lyft1_vec, uber1_vec, uber2_vec)\n",
    "print('')\n",
    "print('idfをつけた場合')\n",
    "print(\"cos(uber1_vec, uber1_vec)\", end=\": \")\n",
    "print(cos(uber1_idf_vec, uber1_idf_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, uber2_vec)\", end=\": \")\n",
    "print(cos(uber1_idf_vec, uber2_idf_vec))\n",
    "\n",
    "print(\"cos(uber2_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber2_idf_vec, lyft1_idf_vec))\n",
    "\n",
    "print(\"cos(uber1_vec, lyft1_vec)\", end=\": \")\n",
    "print(cos(uber1_idf_vec, lyft1_idf_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idfをつけなかった場合\n",
      "cos(realestate1_vec, realestate1_vec): 1.0\n",
      "cos(realestate1_vec, realestate2_vec): 0.4483906358282693\n",
      "cos(realestate2_vec, sports1_vec): 0.11821621346237848\n",
      "cos(realestate1_vec, sports1_vec): 0.1559997520720196\n",
      "\n",
      "idfをつけた場合\n",
      "cos(realestate1_vec, realestate1_vec): 1.0\n",
      "cos(realestate1_vec, realestate2_vec): 0.2152572818734065\n",
      "cos(realestate2_vec, sports1_vec): 0.036037498507822355\n",
      "cos(realestate1_vec, sports1_vec): 0.04986857554914075\n"
     ]
    }
   ],
   "source": [
    "realestate1_list = abstract_not_common_word(word_list('realestate1.txt'))\n",
    "realestate2_list = abstract_not_common_word(word_list('realestate2.txt'))\n",
    "sports1_list = abstract_not_common_word(word_list('sports1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec = count_f(realestate1_list, all_word_list)\n",
    "realestate2_vec = count_f(realestate2_list, all_word_list)\n",
    "sports1_vec = count_f(sports1_list, all_word_list)\n",
    "print('idfをつけなかった場合')\n",
    "print(\"cos(realestate1_vec, realestate1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, realestate2_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate2_vec))\n",
    "\n",
    "print(\"cos(realestate2_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate2_vec, sports1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, sports1_vec))\n",
    "realestate1_list = abstract_not_common_word(word_list('realestate1.txt'))\n",
    "realestate2_list = abstract_not_common_word(word_list('realestate2.txt'))\n",
    "sports1_list = abstract_not_common_word(word_list('sports1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec_tmp = count_f(realestate1_list, all_word_list)\n",
    "realestate2_vec_tmp = count_f(realestate2_list, all_word_list)\n",
    "sports1_vec_tmp = count_f(sports1_list, all_word_list)\n",
    "\n",
    "realestate1_vec = idf(realestate1_vec_tmp, realestate2_vec_tmp, sports1_vec_tmp)\n",
    "realestate2_vec = idf(realestate2_vec_tmp, realestate1_vec_tmp, sports1_vec_tmp)\n",
    "sports1_vec = idf(sports1_vec_tmp, realestate1_vec_tmp, realestate2_vec_tmp)\n",
    "print('')\n",
    "print('idfをつけた場合')\n",
    "print(\"cos(realestate1_vec, realestate1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, realestate2_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, realestate2_vec))\n",
    "\n",
    "print(\"cos(realestate2_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate2_vec, sports1_vec))\n",
    "\n",
    "print(\"cos(realestate1_vec, sports1_vec)\", end=\": \")\n",
    "print(cos(realestate1_vec, sports1_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よりrealestated同士が近づいた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問69<br>\n",
    "LSIを行ってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taishieguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentece_list(file_name):\n",
    "    sentence_list = []\n",
    "    with open(file_name) as f:\n",
    "        for s_line in f:\n",
    "            sentence_list.append(s_line)\n",
    "    return sentence_list\n",
    "\n",
    "def make_x(file_name):\n",
    "    sentence_list = sentece_list(file_name)\n",
    "    all_sentence_list = []\n",
    "    all_word_list = []\n",
    "    for sentence in sentence_list:\n",
    "        not_symbol_list = abstract_not_symbol(sentence_divide(sentence))\n",
    "        if not_symbol_list.size != 0:\n",
    "            all_word_list.append(not_symbol_list)\n",
    "            all_sentence_list.append(not_symbol_list)\n",
    "    all_sentence_list = np.hstack(all_sentence_list)\n",
    "    x_list = []\n",
    "    for sentence in  all_word_list:\n",
    "        x_list.append(count_f(sentence, all_sentence_list))\n",
    "    return np.array(x_list).T\n",
    "\n",
    "def abstract_not_symbol(target_list):\n",
    "    res_list = []\n",
    "    for i in target_list:\n",
    "        res_list.append(i.isalpha())\n",
    "    return target_list[res_list]\n",
    "        \n",
    "def sentence_divide(sentence):\n",
    "    return np.array(nltk.word_tokenize(sentence))\n",
    "\n",
    "def count_f(word_list, all_word_list):\n",
    "    count_list = []\n",
    "    for i in range(all_word_list.size):\n",
    "        word = all_word_list[i]\n",
    "        count = np.count_nonzero(word_list == word)\n",
    "        count_list.append(count)\n",
    "    return np.array(count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセンテンスをSVD分解してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 1 0]]\n",
      "U: [[-5.47722558e-01 -9.28133183e-18 -1.17457637e-17]\n",
      " [-5.47722558e-01 -9.28133183e-18 -1.17457637e-17]\n",
      " [-1.82574186e-01 -7.07106781e-01  4.08248290e-01]\n",
      " [-1.82574186e-01  5.22860278e-17 -8.16496581e-01]\n",
      " [-5.47722558e-01 -2.84412419e-17  7.08520558e-17]\n",
      " [-1.82574186e-01  7.07106781e-01  4.08248290e-01]]\n",
      "s: [3.16227766 1.         1.        ]\n",
      "V: [[-0.57735027 -0.57735027 -0.57735027]\n",
      " [-0.          0.70710678 -0.70710678]\n",
      " [-0.81649658  0.40824829  0.40824829]]\n",
      "[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [-3.46795453e-17  7.28509239e-17  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.01294450e-16 -9.72785220e-18]\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [-5.50525174e-17  1.00000000e+00  2.75262587e-17]]\n",
      "[[ 0.3  0.3  0.1  0.1  0.3  0.1]\n",
      " [ 0.3  0.3  0.1  0.1  0.3  0.1]\n",
      " [ 0.1  0.1  0.7 -0.3  0.1 -0.3]\n",
      " [ 0.1  0.1 -0.3  0.7  0.1 -0.3]\n",
      " [ 0.3  0.3  0.1  0.1  0.3  0.1]\n",
      " [ 0.1  0.1 -0.3 -0.3  0.1  0.7]]\n",
      "[[ 1.00000000e+00  5.89014774e-17  6.62768797e-17]\n",
      " [ 5.89014774e-17  1.00000000e+00 -3.70692371e-17]\n",
      " [ 6.62768797e-17 -3.70692371e-17  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "d1 = 'I like computer.'\n",
    "d2 = 'I like machine.'\n",
    "d3 = 'I like apple.'\n",
    "sentence_divide(d1)\n",
    "sentence_divide(d2)\n",
    "sentence_divide(d3)\n",
    "\n",
    "all_word_list = np.unique(np.hstack((sentence_divide(d1) , sentence_divide(d2), sentence_divide(d3))))\n",
    "l1 = count_f(sentence_divide(d1), all_word_list)\n",
    "l2 = count_f(sentence_divide(d2), all_word_list)\n",
    "l3 = count_f(sentence_divide(d3), all_word_list)\n",
    "all_list = [l1, l2, l3]\n",
    "x = np.array(all_list).T\n",
    "\n",
    "print(x)\n",
    "U, s, V = np.linalg.svd(x, full_matrices=False)\n",
    "print('U', end=': ')\n",
    "print(U)\n",
    "print('s', end=': ')\n",
    "print(s)\n",
    "print('V', end=': ')\n",
    "print(V)\n",
    "print(np.dot(np.dot(U, np.diag(s)), V))\n",
    "print(np.dot(U, U.T))\n",
    "print(np.dot(V, V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の論文をSVDしてみる。\n",
    "https://queue.acm.org/detail.cfm?id=3321612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 1 1]\n",
      " [0 1 0 ... 0 1 1]]\n",
      "[6.47939674e+02 2.07666070e+02 1.30352921e+02 1.06857142e+02\n",
      " 1.02528556e+02 7.83366701e+01 7.55658842e+01 6.72394660e+01\n",
      " 5.86674306e+01 5.66937976e+01 5.22863044e+01 5.00232963e+01\n",
      " 4.71757941e+01 4.51682048e+01 3.98331196e+01 3.89736709e+01\n",
      " 3.64840960e+01 3.51670497e+01 3.29756341e+01 3.25784043e+01\n",
      " 3.03715948e+01 2.98550849e+01 2.90008952e+01 2.79982640e+01\n",
      " 2.71358128e+01 2.64232907e+01 2.58010643e+01 2.49118691e+01\n",
      " 2.45103435e+01 2.29111878e+01 2.23604557e+01 2.16685174e+01\n",
      " 2.11064110e+01 2.05447637e+01 1.91634958e+01 1.87057488e+01\n",
      " 1.82491282e+01 1.77822217e+01 1.70118288e+01 1.67673398e+01\n",
      " 1.64170213e+01 1.56815648e+01 1.53665556e+01 1.43682886e+01\n",
      " 1.43224346e+01 1.39919834e+01 1.37865953e+01 1.32528006e+01\n",
      " 1.30822670e+01 1.22250656e+01 1.20407769e+01 1.19585102e+01\n",
      " 1.14596965e+01 1.14409308e+01 1.06636034e+01 1.02592325e+01\n",
      " 9.91478456e+00 9.85118211e+00 9.70690834e+00 9.30555175e+00\n",
      " 9.19287107e+00 8.65940716e+00 8.43741992e+00 8.19365995e+00\n",
      " 7.69205443e+00 7.58361434e+00 7.18100204e+00 7.05900241e+00\n",
      " 6.92594887e+00 6.34661423e+00 6.30111804e+00 6.19890266e+00\n",
      " 6.06190867e+00 5.62712677e+00 5.48806509e+00 4.83580054e+00\n",
      " 4.48147509e+00 4.02088519e+00 3.59980347e+00 3.45225037e+00\n",
      " 3.09181564e+00 2.94082927e+00 2.80410257e+00 2.44702306e+00\n",
      " 2.16108453e+00 2.11120942e+00 1.93631056e+00 1.53733283e+00\n",
      " 1.41421356e+00 1.00000000e+00 5.19077655e-14 9.78715931e-15]\n"
     ]
    }
   ],
   "source": [
    "x = make_x('cs_article.txt')\n",
    "print(x)\n",
    "U, s, V = np.linalg.svd(x, full_matrices=False)\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
