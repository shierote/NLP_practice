{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問68 英語の文書のcos類似度の比較\n",
    "uberについて書かれた記事2本とlyftについて書かれた記事1本の類似度を比較してみる。\n",
    "- [uber1.txt](https://medium.com/free-code-camp/dark-genius-how-programmers-at-uber-volkswagen-and-zenefits-helped-their-employers-break-the-law-b7a7939c6591)\n",
    "\n",
    "- [uber2.txt](https://medium.com/sandpapersuit/side-hustle-as-a-sign-of-the-apocalypse-e7027a889fc2)\n",
    "\n",
    "- [lyft1.txt](https://medium.com/@johnzimmer/all-lyft-rides-are-now-carbon-neutral-55693af04f36)\n",
    "\n",
    "文章の量を一致させるため一部削っている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taishieguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from functools import reduce\n",
    "import collections\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word_list(file_name):\n",
    "    word_list1 = []\n",
    "    with open(file_name) as f:\n",
    "        for s_line in f:\n",
    "            word_list1.append(nltk.word_tokenize(s_line))\n",
    "    word_list1 = reduce(lambda a, b: a + b, word_list1)\n",
    "    return word_list1\n",
    "\n",
    "def count_f(word_list, all_word_list):\n",
    "    count_list = []\n",
    "    for i in range(all_word_list.size):\n",
    "        word = all_word_list[i]\n",
    "        count = np.count_nonzero(word_list == word)\n",
    "#         t1 = (word, count)\n",
    "        t1 = count\n",
    "        count_list.append(t1)\n",
    "    return count_list\n",
    "\n",
    "def cos(f, g):\n",
    "    return np.dot(f, g)/(np.linalg.norm(f)*np.linalg.norm(g))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber1_list = np.array(word_list('uber1.txt'))\n",
    "uber2_list = np.array(word_list('uber2.txt'))\n",
    "lyft1_list = np.array(word_list('lyft1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((uber1_list, uber2_list, lyft1_list)))\n",
    "\n",
    "uber1_vec = np.array(count_f(uber1_list, all_word_list))\n",
    "uber2_vec = np.array(count_f(uber2_list, all_word_list))\n",
    "lyft1_vec = np.array(count_f(lyft1_list, all_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, uber1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6395783804293877"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, uber2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849740392966016"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber2_vec, lyft1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263543672138268"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(uber1_vec, lyft1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uber1とuber2の値が近くなることを期待したが精度はイマイチであった（むしろlyftとuberの方が近づいてしまっている）。\n",
    "\n",
    "別の文書(newstimesの記事)で比較してみる。\n",
    "\n",
    "- [realestate1.txt](https://medium.com/free-code-camp/dark-genius-how-programmers-at-uber-volkswagen-and-zenefits-helped-their-employers-break-the-law-b7a7939c6591)\n",
    "\n",
    "- [realestate2.txt](https://medium.com/sandpapersuit/side-hustle-as-a-sign-of-the-apocalypse-e7027a889fc2)\n",
    "\n",
    "- [sports1.txt](https://medium.com/@johnzimmer/all-lyft-rides-are-now-carbon-neutral-55693af04f36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "realestate1_list = np.array(word_list('realestate1.txt'))\n",
    "realestate2_list = np.array(word_list('realestate2.txt'))\n",
    "sports1_list = np.array(word_list('sports1.txt'))\n",
    "\n",
    "all_word_list = np.unique(np.hstack((realestate1_list, realestate2_list, sports1_list)))\n",
    "\n",
    "realestate1_vec = np.array(count_f(realestate1_list, all_word_list))\n",
    "realestate2_vec = np.array(count_f(realestate2_list, all_word_list))\n",
    "sports1_vec = np.array(count_f(sports1_list, all_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, realestate1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383622904249884"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, realestate2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632852370301972"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate1_vec, sports1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255375589022625"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(realestate2_vec, sports1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不動産の記事とスポーツの記事だが、きちんとその類似度を分類できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りの課題については締め切りに間に合わなかったため以下のリンクに貼った。<br>\n",
    "https://github.com/shierote/NLP_practice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
